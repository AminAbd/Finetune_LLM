{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1141b23",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**\n",
    "\n",
    "Import necessary libraries for data processing, dataset handling, and tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdcc8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f401c5f",
   "metadata": {},
   "source": [
    "## **Initialize Tokenizer**\n",
    "\n",
    "Load the tokenizer for the EleutherAI/pythia-70m model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef296a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21862b7",
   "metadata": {},
   "source": [
    "## **Example Text**\n",
    "\n",
    "Create a simple example text to demonstrate tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791b2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f39494",
   "metadata": {},
   "source": [
    "## **Encode Text**\n",
    "\n",
    "Tokenize the example text and get the input IDs (token IDs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41547d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12764, 13, 849, 403, 368, 32]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ef60b",
   "metadata": {},
   "source": [
    "## **Decode Tokens**\n",
    "\n",
    "Convert the token IDs back to text to verify the tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff0acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens back into text:  Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd72bde",
   "metadata": {},
   "source": [
    "## **Tokenize Multiple Texts**\n",
    "\n",
    "Tokenize multiple texts at once to see how the tokenizer handles batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bdcec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d6d59",
   "metadata": {},
   "source": [
    "## **Apply Padding**\n",
    "\n",
    "Use padding to make all sequences the same length (padding tokens added to shorter sequences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fb492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73869b",
   "metadata": {},
   "source": [
    "## **Apply Truncation**\n",
    "\n",
    "Truncate sequences to a maximum length of 3 tokens (from the right side by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c399f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation:  [[12764, 13, 849], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f05ad",
   "metadata": {},
   "source": [
    "## **Left-Side Truncation**\n",
    "\n",
    "Change truncation side to the left, so tokens are removed from the beginning of the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0319a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using left-side truncation:  [[403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8842dd",
   "metadata": {},
   "source": [
    "## **Padding and Truncation Combined**\n",
    "\n",
    "Apply both padding and truncation together to handle variable-length sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4245fe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4c986",
   "metadata": {},
   "source": [
    "# **Prepare instruction dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbaa9f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One datapoint in the finetuning dataset:\n",
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset directly from Hugging Face (no manual download needed)\n",
    "dataset = load_dataset(\"kotzeje/lamini_docs.jsonl\")\n",
    "instruction_dataset_df = dataset[\"train\"].to_pandas()\n",
    "examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4becef7",
   "metadata": {},
   "source": [
    "# **Show the Dataset Length, and the First five examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1447bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs/examples in the dataset: 1400\n",
      "\n",
      "First 5 examples from the dataset:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Question: How can I evaluate the performance and quality of the generated text from Lamini models?\n",
      "Answer: There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Question: Can I find information about the code's approach to handling long-running tasks and background jobs?\n",
      "Answer: Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Question: How does Lamini AI handle requests for generating text that requires reasoning or decision-making based on given information?\n",
      "Answer: Lamini AI offers features for generating text that requires logical reasoning or inference beyond simple text generation. It can handle user prompts that involve complex reasoning or logical inference, and can generate text that captures the nuances of different cultural or regional variations.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Question: Does the `submit_job()` function expose any advanced training options such as learning rate schedules or early stopping?\n",
      "Answer: It is unclear which `submit_job()` function is being referred to as there is no such function defined in Laminiâ€™s python library snippets. Please provide more information or context to answer the question accurately.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Question: Does the `add_data()` function support different data augmentation techniques or preprocessing options for training data?\n",
      "Answer: No, the `add_data()` function does not support different data augmentation techniques or preprocessing options for training data. It simply adds the provided examples to the program's list of examples.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print dataset information\n",
    "print(f\"Number of pairs/examples in the dataset: {len(instruction_dataset_df)}\")\n",
    "print(f\"\\nFirst 5 examples from the dataset:\\n\")\n",
    "print(\"=\"*80)\n",
    "for i in range(min(5, len(instruction_dataset_df))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {examples['question'][i]}\")\n",
    "    print(f\"Answer: {examples['answer'][i]}\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819d2ff",
   "metadata": {},
   "source": [
    "# **Tokenize a single example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad7d8515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  2347   476   309  7472   253  3045   285  3290\n",
      "    273   253  4561  2505   432   418  4988    74  3210    32   187   187\n",
      "   4118 37741    27  2512   403  2067 17082   326   476   320   908   281\n",
      "   7472   253  3045   285  3290   273  4561  2505   432   418  4988    74\n",
      "   3210    13  1690 44229   414    13   378  1843    54  4868    13   285\n",
      "   1966  7103    15  3545 12813   414  5593   849   973   253  1566 26295\n",
      "    253  1735  3159   275   247  3425    13  1223   378  1843    54  4868\n",
      "   5593   253 14259   875   253  4561  2505   285   247  3806  2505    15\n",
      "   8801  7103  8687  1907  1966 16006  2281   253  3290   273   253  4561\n",
      "   2505  1754   327  2616   824   347 25253    13  2938  1371    13   285\n",
      "  17200    15   733   310  8521   281   897   247  5019   273   841 17082\n",
      "    323   247 11088  7103   273   253  1566   434  3045    15]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55cea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c928a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "207805fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  2347,   476,   309,  7472,   253,\n",
       "         3045,   285,  3290,   273,   253,  4561,  2505,   432,   418,\n",
       "         4988,    74,  3210,    32,   187,   187,  4118, 37741,    27,\n",
       "         2512,   403,  2067, 17082,   326,   476,   320,   908,   281,\n",
       "         7472,   253,  3045,   285,  3290,   273,  4561,  2505,   432,\n",
       "          418,  4988,    74,  3210,    13,  1690, 44229,   414,    13,\n",
       "          378,  1843,    54,  4868,    13,   285,  1966,  7103,    15,\n",
       "         3545, 12813,   414,  5593,   849,   973,   253,  1566, 26295,\n",
       "          253,  1735,  3159,   275,   247,  3425,    13,  1223,   378,\n",
       "         1843,    54,  4868,  5593,   253, 14259,   875,   253,  4561,\n",
       "         2505,   285,   247,  3806,  2505,    15,  8801,  7103,  8687,\n",
       "         1907,  1966, 16006,  2281,   253,  3290,   273,   253,  4561,\n",
       "         2505,  1754,   327,  2616,   824,   347, 25253,    13,  2938,\n",
       "         1371,    13,   285, 17200,    15,   733,   310,  8521,   281,\n",
       "          897,   247,  5019,   273,   841, 17082,   323,   247, 11088,\n",
       "         7103,   273,   253,  1566,   434,  3045,    15]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087e139",
   "metadata": {},
   "source": [
    "# **Tokenize the instruction dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73251d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14394bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774c26d5bd74cb4987070982a2edcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert the finetuning_dataset list to a Hugging Face Dataset object\n",
    "finetuning_dataset_loaded = datasets.Dataset.from_list(finetuning_dataset)\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18a870c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50080c75",
   "metadata": {},
   "source": [
    "# **Prepare test/train splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2e27568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
